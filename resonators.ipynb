{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "E6R7nSA6c8sf",
        "Ij79EHtaNDCt",
        "CUOAr6RddAdK"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prelude\n",
        "\n"
      ],
      "metadata": {
        "id": "uwIzSYF_iLxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[**Resonators**](https://github.com/0xmaddie/0xmaddie/blob/main/resonators)\n",
        "is a Python library that uses data races and particle swarm dynamics\n",
        "to approximate functions. You can train a resonator like a transformer\n",
        "but it uses Python objects instead of vectors.\n",
        "\n",
        "## Particle swarms and oracle machines\n",
        "How does \"swarm intelligence\" work? A logical dependence on an\n",
        "implicit function representation observed by repeated measurement of\n",
        "collective behavior is a type of non-algorithmic computation because\n",
        "there's no effective procedure that would give you that information.\n",
        "\n",
        "You can use a particle swarm to train something called a component\n",
        "system — like a Turing machine or a bunch of Python or Lisp functions\n",
        "— in a manner similar to a neural net. The idea is to use the\n",
        "priorities of racing threads as an implicit representation of an\n",
        "energy function learned from a dataset.\n",
        "\n",
        "The important part is that each individual thread operation can only\n",
        "use data that is averaged across every thread. This provides an\n",
        "analogy to continuity and allows you to pass to the mean field limit,\n",
        "so you can optimize in a manner similar to a neural net.\n",
        "\n",
        "Imagine a class of programs written as lists of rules of the following\n",
        "form: If the state has a certain property, transform the state in a\n",
        "certain way. To execute these programs, evaluate the conditions in the\n",
        "order they are written until one is true, and then apply the\n",
        "corresponding transformation.\n",
        "\n",
        "We'd like to add weights to the space of all possible programs of this\n",
        "shape and tune them in alignment with a dataset. In other words, we'd\n",
        "like to create an oracle machine. The resonator is a machine of this\n",
        "type that uses an analogy to continuous functions through particle\n",
        "swarm dynamics to explore this space of programs.\n",
        "\n",
        "The resonator contains lists of every possible property and\n",
        "transformation, respectively called the \"inputs\" and the\n",
        "\"outputs\". When a layer is presented with a swarm of states, the\n",
        "inputs race according to their advice. The advice is provided in the\n",
        "form of the bias of a coin; on each step of the race the coins\n",
        "represent the probability that the respective input will be next in\n",
        "line until the inputs are sorted.\n",
        "\n",
        "Then, in the sorted order, the inputs attempt to provide a measurement\n",
        "of the swarm states that satisfy some property. If an input is unable\n",
        "to provide the minimum number of states required for a measurement,\n",
        "then the search proceeds with the next input in line. If no inputs are\n",
        "able to provide a measurement, the procedure raises an exception.\n",
        "\n",
        "If the inputs are able to provide a measurement, then the outputs\n",
        "condition on the winner of the input race and perform their own race\n",
        "in the same manner. The winner of the output race then combines the\n",
        "entire measurement of states into a single state in a manner similar\n",
        "to a crossover operation from genetic programming. It's important that\n",
        "the output's final state depends on many states sampled from the swarm\n",
        "at once in order to provide the analogy to continuity.\n",
        "\n",
        "This input-output process is repeated until a measurement of states\n",
        "has been provided by the outputs. Then, this measurement races with\n",
        "the initial states according to the advice as a type of residual\n",
        "connection. The idea is that the probabilities of all of these events\n",
        "are an implicit representation of a function learned from a\n",
        "dataset. In this sense, the coins are an oracle provided to a machine\n",
        "allowing it to self-organize.\n"
      ],
      "metadata": {
        "id": "XBeyrgZNNEMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google\n",
        "google.colab.drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/secret.py .\n",
        "import secret"
      ],
      "metadata": {
        "id": "q-aQyEETzQUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "SjxzCm_80O4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login()"
      ],
      "metadata": {
        "id": "LOw4lMy6552G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construction"
      ],
      "metadata": {
        "id": "7ry3CWWxMPCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "8OZwrT4Ij_9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Decoder:\n",
        "  def __init__(self, inputs, outputs, weights):\n",
        "    self.inputs  = inputs\n",
        "    self.outputs = outputs\n",
        "    self.weights = weights\n",
        "\n",
        "  @property\n",
        "  def density(self):\n",
        "    return len(self.inputs)*len(self.outputs)\n",
        "\n",
        "  # The number of weights per layer.\n",
        "  @property\n",
        "  def layer_capacity(self):\n",
        "    return len(self.inputs) + len(self.inputs) * len(self.outputs) + 1\n",
        "\n",
        "  @property\n",
        "  def layers(self):\n",
        "    return len(self.weights)/self.layer_capacity\n",
        "\n",
        "  @property\n",
        "  def capacity(self):\n",
        "    return len(self.weights)\n",
        "\n",
        "  # The weights associated with the inputs for a certain layer.\n",
        "  def win(self, layer):\n",
        "    lhs = layer * self.capacity\n",
        "    return self.weights[lhs:lhs+len(self.inputs)]\n",
        "\n",
        "  # The weights associated with the outputs for a certain layer.\n",
        "  def wout(self, layer, input_index):\n",
        "    input_lhs  = layer * self.capacity\n",
        "    output_lhs = input_lhs + len(self.inputs)\n",
        "    target_lhs = output_lhs + input_index * len(self.outputs)\n",
        "    return self.weights[target_lhs:target_lhs+len(self.outputs)]\n",
        "\n",
        "  # The weights associated with the residual for a certain layer.\n",
        "  def wres(self, layer):\n",
        "    input_lhs    = layer * self.capacity\n",
        "    residual_idx = input_lhs + len(self.inputs) + len(self.inputs) * len(self.outputs)\n",
        "    return self.weights[residual_idx]\n",
        "\n",
        "  def sort(self, components, weights):\n",
        "    # Each weight is the probability that the corresponding\n",
        "    # component will be next in line.\n",
        "    indices = np.random.choice(\n",
        "      len(inputs),\n",
        "      size=len(inputs),\n",
        "      replace=False,\n",
        "      # TODO: softmax the weights here to make\n",
        "      # sure they always sum to 1\n",
        "      p=weights,\n",
        "    )\n",
        "    return indices\n",
        "\n",
        "  def __call__(self, swarm):\n",
        "    for time in range(self.layers):\n",
        "      inputw    = self.win(time)\n",
        "      residualw = self.wres(time)\n",
        "      hidden    = []\n",
        "      for i in range(self.purity):\n",
        "        # The inputs race to provide a measurement.\n",
        "        inputbest = None\n",
        "        for inputid in self.sort(self.inputs, inputw):\n",
        "          input  = self.inputs[inputid]\n",
        "          sample = []\n",
        "          for state in swarm:\n",
        "            if input(state):\n",
        "              sample.append(state)\n",
        "            if len(sample) >= threshold:\n",
        "              inputbest = inputid\n",
        "              break\n",
        "        # If no inputs were able to provide a large enough measurement\n",
        "        # then an exception is raised.\n",
        "        if len(sample) < threshold:\n",
        "          raise ValueError()\n",
        "        activity = False\n",
        "        # The outputs condition on the winner of the race\n",
        "        # and perform their own race to provide a state.\n",
        "        outputw  = self.wout(time, inputbest)\n",
        "        for outputid in self.sort(self.outputs, outputw):\n",
        "          output = self.outputs[outputid]\n",
        "          try:\n",
        "            # The output is applied to *all* of the sample values.\n",
        "            # It's important to mix states across the swarm to allow\n",
        "            # the analogy to continuity.\n",
        "            point    = output(sample)\n",
        "            activity = True\n",
        "            hidden.append(point)\n",
        "            break\n",
        "          except OutputError:\n",
        "            pass\n",
        "        if not activity:\n",
        "          raise ValueError()\n",
        "      # The final states race with the initial states.\n",
        "      target = []\n",
        "      for i in range(self.purity):\n",
        "        if np.random.random() < residualw:\n",
        "          state = np.random.choice(hidden)\n",
        "        else:\n",
        "          state = np.random.choice(swarm)\n",
        "        target.append(state)\n",
        "      swarm = target\n",
        "    return swarm"
      ],
      "metadata": {
        "id": "lX3hKo6RkB7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "class Llama:\n",
        "  tokenizer: transformers.AutoTokenizer\n",
        "  model: transformers.AutoModelForCausalLM\n",
        "  system_prompt: str\n",
        "  temperature: float\n",
        "  max_new_tokens: int\n",
        "  terminators: list\n",
        "  purity: int\n",
        "  quota: int\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    system_prompt: str = 'You are a helpful assistant.',\n",
        "    temperature: float = 0.7,\n",
        "    max_new_tokens: int = 4096,\n",
        "    purity: int = 100,\n",
        "    quota: int = 1000,\n",
        "  ):\n",
        "    model_name     = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "      model_name,\n",
        "      #cache_dir=secret.home,\n",
        "      #local_files_only=True,\n",
        "    )\n",
        "    self.model     = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      device_map=\"auto\",\n",
        "      load_in_8bit=True,\n",
        "      #cache_dir=secret.home,\n",
        "      #local_files_only=True,\n",
        "    )\n",
        "    self.terminators = [\n",
        "      self.tokenizer.eos_token_id,\n",
        "      self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "    self.system_prompt  = system_prompt\n",
        "    self.temperature    = temperature\n",
        "    self.max_new_tokens = max_new_tokens\n",
        "    self.purity         = purity\n",
        "    self.quota          = quota\n",
        "\n",
        "  def __apply_chat_template(self, state):\n",
        "    prompt = [{ 'role': 'system', 'content': self.system_prompt }]\n",
        "    for index, content in enumerate(state):\n",
        "      if index%2 == 0:\n",
        "        role = 'user'\n",
        "      else:\n",
        "        role = 'assistant'\n",
        "      prompt.append({ 'role': role, 'content': content })\n",
        "    print(f'<prompt>{prompt}</prompt>')\n",
        "    input_ids = self.tokenizer.apply_chat_template(\n",
        "      prompt,\n",
        "      #tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors='pt',\n",
        "    ).to(self.model.device)\n",
        "\n",
        "    return input_ids\n",
        "\n",
        "  def get(self, state):\n",
        "    source_ids = self.__apply_chat_template(state)\n",
        "    targets = self.model.generate(\n",
        "      source_ids,\n",
        "      max_new_tokens=self.max_new_tokens,\n",
        "      do_sample=True,\n",
        "      temperature=self.temperature,\n",
        "      eos_token_id=self.terminators,\n",
        "    )\n",
        "    target_ids = targets[0][source_ids.shape[-1]:]\n",
        "    target = self.tokenizer.decode(target_ids, skip_special_tokens=True)\n",
        "    return target\n",
        "\n",
        "  def reduce(self, input, map, reduce):\n",
        "    quota = self.quota\n",
        "    samples = []\n",
        "    while quota > 0 and len(samples) < self.purity:\n",
        "      quota -= 1\n",
        "      try:\n",
        "        object = map(self.get(input))\n",
        "        samples.append(object)\n",
        "      except ValueError:\n",
        "        pass\n",
        "    if len(samples) < self.purity and quota == 0:\n",
        "      raise ValueError(f'quota consumed')\n",
        "    output = reduce(samples)\n",
        "    return output"
      ],
      "metadata": {
        "id": "94e8XmihvlVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "  system_prompt=f'''\n",
        "You are a helpful assistant.\n",
        "'''.strip(),\n",
        "  temperature=0.7,\n",
        "  max_new_tokens=1024,\n",
        "  purity=1,\n",
        "  quota=3,\n",
        ")"
      ],
      "metadata": {
        "id": "8gcxnxAolx6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_tag_body(document, tag_name):\n",
        "  # Perform string replacement to transform <value> tags into <div class=\"value\">\n",
        "  document = document.replace(f\"<{tag_name}>\", f'<div class=\"{tag_name}\">').replace(f\"</{tag_name}>\", \"</div>\")\n",
        "  # Parse the modified HTML document with BeautifulSoup\n",
        "  soup = BeautifulSoup(document, 'html.parser')\n",
        "  # Find all divs with the class \"value\"\n",
        "  value_divs = soup.find_all('div', class_=tag_name)\n",
        "  # Ensure there is exactly one div with class \"value\"\n",
        "  if len(value_divs) != 1:\n",
        "    raise ValueError(f\"Expected exactly one <div class='{tag_name}'> tag, but found {len(value_divs)}.\")\n",
        "  # Return the text content of the <div class=\"value\">\n",
        "  return value_divs[0].get_text()\n",
        "\n",
        "class Repository:\n",
        "  llm: Llama\n",
        "\n",
        "  def __init__(self, llm):\n",
        "    self.llm = llm\n",
        "\n",
        "  def input(self, predicate):\n",
        "    def map_input(x):\n",
        "      # get_tag_body uses BeautifulSoup to\n",
        "      # 1. Replace <value> tags with <div class=\"value\"> with string replacement.\n",
        "      # 2. Ensure there is only one such tag, raising a ValueError if this is not true.\n",
        "      # 3. Return the body of that tag as a string.\n",
        "      print(f'checking bool in\\n{x}')\n",
        "      value = get_tag_body(x, 'value')\n",
        "      print(f'value = {value}')\n",
        "      return bool(value)\n",
        "    def reduce_input(xs):\n",
        "      numbers = [1 if bool(x) else 0 for x in xs]\n",
        "      average = sum(numbers)/len(xs)\n",
        "      print(f'average = {average}')\n",
        "      return average > 0.5\n",
        "    def body(state):\n",
        "      prompt = f'''\n",
        "You are analyzing prompts for a generative text-to-image model.\n",
        "Your task is to determine whether this predicate:\n",
        "\n",
        "```\n",
        "{predicate}\n",
        "```\n",
        "\n",
        "is true of the following prompt:\n",
        "\n",
        "```\n",
        "{state}\n",
        "```\n",
        "\n",
        "First, provide a comprehensive, detailed, and nuanced analysis of\n",
        "whether the predicate is true of the prompt. Then, place the word\n",
        "True or the word False within <value> tags.\n",
        "'''.strip()\n",
        "      response = self.llm.reduce([prompt], map_input, reduce_input)\n",
        "      return response\n",
        "    return body\n",
        "\n",
        "  def output(self, transform):\n",
        "    def map_output(x):\n",
        "      value = get_tag_body(x, 'value')\n",
        "      return value\n",
        "    def reduce_output(xs):\n",
        "      return xs[0]\n",
        "    def body(input, sample):\n",
        "      prompt = f'''\n",
        "You are creating prompts for a generative text-to-image model.\n",
        "Your task is to crossover and mutate prompts that have a certain relation.\n",
        "\n",
        "Consider the following text-to-image prompts:\n",
        "\n",
        "```\n",
        "{sample}\n",
        "```\n",
        "\n",
        "All of these prompts satisfy the following relation:\n",
        "\n",
        "```\n",
        "{input}\n",
        "```\n",
        "\n",
        "Your task is to both combine and mutate these prompts\n",
        "according to the following principle:\n",
        "\n",
        "```\n",
        "{transform}\n",
        "```\n",
        "\n",
        "First, to perform your crossover operation, combine all of\n",
        "the prompts in to a single prompt based on some theme you notice.\n",
        "Then, determine how you will modify the prompt according to the\n",
        "principle given to you. When you have produced a final prompt,\n",
        "place it within <value> tags.\n",
        "'''.strip()\n",
        "      response = self.llm.reduce([prompt], map_output, reduce_output)\n",
        "      return response\n",
        "    return body"
      ],
      "metadata": {
        "id": "BH-kHssBly7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo = Repository(llm)\n",
        "\n",
        "inputs = [\n",
        "  repo.input('Is this a classical painting?'),\n",
        "  repo.input('Is this an anime illustration?'),\n",
        "  repo.input('Does this artwork belong to the Renaissance period?'),\n",
        "  repo.input('Is this a modern abstract artwork?'),\n",
        "  repo.input('Is this piece in the style of Impressionism?'),\n",
        "  repo.input('Does this resemble a Baroque painting?'),\n",
        "  repo.input('Is this artwork in the style of Cubism?'),\n",
        "  repo.input('Is the artist associated with the Surrealism movement?'),\n",
        "  repo.input('Does this image use techniques typical of watercolor painting?'),\n",
        "  repo.input('Is this artwork a digital creation?'),\n",
        "  repo.input('Does this represent a form of street art?'),\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "  repo.output('Make this an anime illustration.'),\n",
        "  repo.output('Make this a classical painting.'),\n",
        "  repo.output('Transform this image to reflect the Renaissance art style.'),\n",
        "  repo.output('Create this image in a modern abstract style.'),\n",
        "  repo.output('Convert this to an Impressionist style painting.'),\n",
        "  repo.output('Reimagine this as a Baroque period painting.'),\n",
        "  repo.output('Adapt this image to the Cubism art style.'),\n",
        "  repo.output('Render this image in a Surrealist style.'),\n",
        "  repo.output('Use watercolor techniques for this image.'),\n",
        "  repo.output('Create this artwork using digital media.'),\n",
        "  repo.output('Style this image as street art.'),\n",
        "]"
      ],
      "metadata": {
        "id": "eCub1npZss8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_classical = inputs[0]\n",
        "fst = 'A digital art illustration by Ilya Kuvshinov.'\n",
        "snd = 'An oil painting by John Singer Sargent'\n",
        "print(f'{fst} = {is_classical(fst)}')\n",
        "print(f'{snd} = {is_classical(snd)}')"
      ],
      "metadata": {
        "id": "u9vBKtmu3gdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_anime = outputs[0]\n",
        "response = make_anime(\n",
        "  'Is this a classical painting?',\n",
        "  [\n",
        "    'An oil painting of a woman holding a parasol by John Singer Sargent',\n",
        "    'A still life of fruits on a table by Joaquin Sorolla',\n",
        "    'A scene from Greek mythology by Leonardo da Vinci',\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "SGrthOvjEWad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "DMgqpFjtFgsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_weights = len(inputs)+len(inputs)*len(outputs)+1\n",
        "decoder = Decoder(\n",
        "  inputs=inputs,\n",
        "  outputs=outputs,\n",
        "  weights=np.random.rand(num_weights),\n",
        ")\n",
        "initial_state = ['a woman holding a parasol', 'an oil painting', 'by ilya kuvshinov']\n",
        "final_states  = decoder(initial_state)\n",
        "print(final)"
      ],
      "metadata": {
        "id": "VrTXkFINx4yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning"
      ],
      "metadata": {
        "id": "CUOAr6RddAdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass(frozen=True)\n",
        "class Optimizer:\n",
        "  facc: float\n",
        "  fres: float\n",
        "  rate: float\n",
        "  decay: float\n",
        "\n",
        "  def __call__(self, pos, acc, grad):\n",
        "    hacc  = self.interp(grad, acc, self.facc)\n",
        "    res   = self.interp(grad, acc, self.fres)\n",
        "    mix   = pos*self.decay+sign(res)\n",
        "    delta = mix*self.rate\n",
        "    return delta, hacc\n",
        "\n",
        "class Tuner:\n",
        "  quota: int\n",
        "  purity: int\n",
        "  density: int\n",
        "  update: callable\n",
        "\n",
        "  def measure(self, machine, data):\n",
        "    energy = 0\n",
        "    for i in range(self.purity):\n",
        "      (init, eval)  = data()\n",
        "      energy       += eval(machine(init))\n",
        "    return energy/self.purity\n",
        "\n",
        "  def __call__(self, init, cons, data):\n",
        "    pos  = []\n",
        "    acc  = []\n",
        "    loss = []\n",
        "    for _ in range(self.quota):\n",
        "      for i in range(self.density):\n",
        "        machine = cons(pos[i])\n",
        "        loss[i] = self.measure(machine, data)\n",
        "      center  = self.average(pos, loss)\n",
        "      teacher = cons(center)\n",
        "      cutoff  = self.measure(teacher, data)\n",
        "      for i in range(self.density):\n",
        "        if loss[i] <= cutoff:\n",
        "          continue\n",
        "        compass  = pos[i]-center\n",
        "        sort     = compass*self.coherence\n",
        "        search   = (compass@compass)*self.noise()\n",
        "        gradient = sort+search\n",
        "        vel, acc = self.update(pos[i], acc[i], gradient)\n",
        "        pos[i]  += vel\n",
        "        acc[i]   = acc\n",
        "    return self.average(pos, loss)"
      ],
      "metadata": {
        "id": "BOHgnO8ib_3o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}